在本学期的“数值分析”课程项目中，我们探究了 CosFormer，LARA，SkyFormer 和 MEGA 这 4 种模型加速计算 Self-Attention 的原理，并实际测试了它们的建模能力，验证了它们在训练和推理过程中的加速效果。实验结果表明，以上 4 种 Self-Attention 的快速计算机制总体来说可以与普通的 Self-Attention 达到类似的长序列建模效果。虽然各有局限性，但它们在较长的序列上，都能较为显著地减少训练和推理过程中的显存开销和时间占用。
