%%%%%%%%%%%% BRGIN OF LAMBDA ADDED
@misc{ma2023mega,
      title={Mega: Moving Average Equipped Gated Attention}, 
      author={Xuezhe Ma and Chunting Zhou and Xiang Kong and Junxian He and Liangke Gui and Graham Neubig and Jonathan May and Luke Zettlemoyer},
      year={2023},
      eprint={2209.10655},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{chen2021skyformer,
    title={Skyformer: Remodel Self-Attention with Gaussian Kernel and Nystr\"om Method}, 
    author={Yifan Chen and 
            Qi Zeng and 
            Heng Ji and 
            Yun Yang},
    booktitle={Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December
               6-14, 2021, virtual},
    year={2021}
}

@misc{feng2023diffuser,
      title={Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences}, 
      author={Aosong Feng and Irene Li and Yuang Jiang and Rex Ying},
      year={2023},
      eprint={2210.11794},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@InProceedings{pmlr-v162-zheng22b,
  title = 	 {Linear Complexity Randomized Self-attention Mechanism},
  author =       {Zheng, Lin and Wang, Chong and Kong, Lingpeng},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {27011--27041},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/zheng22b/zheng22b.pdf}
}


@inproceedings{
  zhen2022cosformer,
  title={cosFormer: Rethinking Softmax In Attention},
  author={Zhen Qin and Weixuan Sun and Hui Deng and Dongxu Li and Yunshen Wei and Baohong Lv and Junjie Yan and Lingpeng Kong and Yiran Zhong},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=Bl8CQrx2Up4}
}

%%%%%%%%%%%% END OF LAMBDA ADDED




%%%%%%%%%%%%% BEGIN OF MEGA
@article{ma2021luna,
  title={Luna: Linear unified nested attention},
  author={Ma, Xuezhe and Kong, Xiang and Wang, Sinong and Zhou, Chunting and May, Jonathan and Ma, Hao and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2441--2453},
  year={2021}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{winters1960forecasting,
  title={Forecasting sales by exponentially weighted moving averages},
  author={Winters, Peter R},
  journal={Management science},
  volume={6},
  number={3},
  pages={324--342},
  year={1960},
  publisher={INFORMS}
}

@article{hunter1986exponentially,
  title={The exponentially weighted moving average},
  author={Hunter, J Stuart},
  journal={Journal of quality technology},
  volume={18},
  number={4},
  pages={203--210},
  year={1986},
  publisher={Taylor \& Francis}
}

@article{hornik1989multilayer,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Elsevier}
}

@inproceedings{goller1996learning,
  title={Learning task-dependent distributed representations by backpropagation through structure},
  author={Goller, Christoph and Kuchler, Andreas},
  booktitle={Neural Networks, 1996., IEEE International Conference on},
  volume={1},
  pages={347--352},
  year={1996},
  organization={IEEE}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{gers2000learning,
  title={Learning to forget: Continual prediction with LSTM},
  author={Gers, Felix A and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  journal={Neural computation},
  volume={12},
  number={10},
  pages={2451--2471},
  year={2000},
  publisher={MIT Press}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and others},
  year={2009},
  journal={Technical Report. University of Toronto},
  publisher={Citeseer}
}

@article{mckenzie2010damped,
  title={Damped trend exponential smoothing: a modelling viewpoint},
  author={McKenzie, Eddie and Gardner Jr, Everette S},
  journal={International Journal of Forecasting},
  volume={26},
  number={4},
  pages={661--665},
  year={2010},
  publisher={Elsevier}
}

@article{radev2013acl,
  title={The ACL anthology network corpus},
  author={Radev, Dragomir R and Muthukrishnan, Pradeep and Qazvinian, Vahed and Abu-Jbara, Amjad},
  journal={Language Resources and Evaluation},
  volume={47},
  number={4},
  pages={919--944},
  year={2013},
  publisher={Springer}
}

@inproceedings{kim-2014-convolutional,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP-2014})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    pages = "1746--1751",
}

@article{cho2014properties,
  title={On the Properties of Neural Machine Translation: Encoder--Decoder Approaches},
  author={Cho, Kyunghyun and van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  journal={Syntax, Semantics and Structure in Statistical Translation},
  pages={103},
  year={2014}
}

@inproceedings{bojar2014findings,
  title={Findings of the 2014 workshop on statistical machine translation},
  author={Bojar, Ond{\v{r}}ej and Buck, Christian and Federmann, Christian and Haddow, Barry and Koehn, Philipp and Leveling, Johannes and Monz, Christof and Pecina, Pavel and Post, Matt and Saint-Amand, Herve and others},
  booktitle={Proceedings of the ninth workshop on statistical machine translation},
  pages={12--58},
  year={2014}
}

@inproceedings{bahdanau2015neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2015}
}

@inproceedings{luong2015effective,
  title={Effective Approaches to Attention-based Neural Machine Translation},
  author={Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D},
  booktitle={Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1412--1421},
  year={2015}
}

@book{svetunkov2016complex,
  title={Complex exponential smoothing},
  author={Svetunkov, Ivan},
  year={2016},
  publisher={Lancaster University (United Kingdom)}
}

@inproceedings{sennrich2016neural,
  title={Neural Machine Translation of Rare Words with Subword Units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1715--1725},
  year={2016}
}

@inproceedings{strubell-etal-2017-fast,
    title = "Fast and Accurate Entity Recognition with Iterated Dilated Convolutions",
    author = "Strubell, Emma  and
      Verga, Patrick  and
      Belanger, David  and
      McCallum, Andrew",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing ({EMNLP-2017})",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    pages = "2670--2680",
}

@inproceedings{gehring2017convolutional,
  title={Convolutional sequence to sequence learning},
  author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
  booktitle={International conference on machine learning (ICML-2017)},
  pages={1243--1252},
  year={2017},
  organization={PMLR}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{ramachandran2017swish,
  title={Swish: a self-gated activation function},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.05941},
  volume={7},
  number={1},
  pages={5},
  year={2017},
  publisher={Technical report}
}

@inproceedings{post-2018-call,
  title = "A Call for Clarity in Reporting {BLEU} Scores",
  author = "Post, Matt",
  booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
  year = "2018",
  address = "Belgium, Brussels",
  publisher = "Association for Computational Linguistics",
  pages = "186--191",
}

@inproceedings{drew2018advances,
 author = {Linsley, Drew and Kim, Junkyung and Veerabadran, Vijay and Windolf, Charles and Serre, Thomas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 publisher = {Curran Associates, Inc.},
 title = {Learning long-range spatial dependencies with horizontal gated recurrent units},
 volume = {31},
 year = {2018}
}

@inproceedings{nangia2018listops,
  title={ListOps: A Diagnostic Dataset for Latent Tree Learning},
  author={Nangia, Nikita and Bowman, Samuel},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop},
  pages={92--99},
  year={2018}
}

@article{elfwing2018sigmoid,
  title={Sigmoid-weighted linear units for neural network function approximation in reinforcement learning},
  author={Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
  journal={Neural Networks},
  volume={107},
  pages={3--11},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{ott2018scaling,
  title={Scaling Neural Machine Translation},
  author={Ott, Myle and Edunov, Sergey and Grangier, David and Auli, Michael},
  booktitle={Proceedings of the Third Conference on Machine Translation: Research Papers},
  pages={1--9},
  year={2018}
}

@inproceedings{parmar2018image,
  title={Image transformer},
  author={Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  booktitle={International Conference on Machine Learning (ICML-2018)},
  pages={4055--4064},
  year={2018},
  organization={PMLR}
}

@inproceedings{ott2019fairseq,
  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
  year = {2019},
}

@inproceedings{dai2019transformer,
  title={Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G and Le, Quoc and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2978--2988},
  year={2019}
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@inproceedings{huang2020improve,
  title={Improve Transformer Models with Better Relative Position Embeddings},
  author={Huang, Zhiheng and Liang, Davis and Xu, Peng and Xiang, Bing},
  booktitle={Findings of the Association for Computational Linguistics (EMNLP-2020)},
  pages={3327--3335},
  year={2020}
}

@inproceedings{rae2020compressive,
  title={Compressive Transformers for Long-Range Sequence Modeling},
  author={Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Hillier, Chloe and Lillicrap, Timothy P},
  booktitle={International Conference on Learning Representations (ICLR-2020)},
  year={2020}
}

@inproceedings{ke2020rethinking,
  title={Rethinking Positional Encoding in Language Pre-training},
  author={Ke, Guolin and He, Di and Liu, Tie-Yan},
  booktitle={International Conference on Learning Representations (ICLR-2020)},
  year={2020}
}

@inproceedings{dosovitskiy2020image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations (ICLR-2020)},
  year={2020}
}

@article{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}

@article{madani2020progen,
  title={ProGen: Language Modeling for Protein Generation},
  author={Madani, Ali and McCann, Bryan and Naik, Nikhil and Keskar, Nitish Shirish and Anand, Namrata and Eguchi, Raphael R and Huang, Possu and Socher, Richard},
  journal={bioRxiv},
  year={2020},
  publisher={Cold Spring Harbor Laboratory}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@inproceedings{parisotto2020stabilizing,
  title={Stabilizing transformers for reinforcement learning},
  author={Parisotto, Emilio and Song, Francis and Rae, Jack and Pascanu, Razvan and Gulcehre, Caglar and Jayakumar, Siddhant and Jaderberg, Max and Kaufman, Raphael Lopez and Clark, Aidan and Noury, Seb and others},
  booktitle={International conference on machine learning},
  pages={7487--7498},
  year={2020},
  organization={PMLR}
}

@article{xu2020transformer,
  title={Transformer with depth-wise lstm},
  author={Xu, Hongfei and Liu, Qiuhui and Xiong, Deyi and van Genabith, Josef},
  journal={arXiv preprint arXiv:2007.06257},
  year={2020}
}

@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{tay2020efficient,
  title={Efficient transformers: A survey},
  author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal={arXiv preprint arXiv:2009.06732},
  year={2020}
}

@article{gu2020hippo,
  title={Hippo: Recurrent memory with optimal polynomial projections},
  author={Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1474--1487},
  year={2020}
}

@article{ma2020apollo,
  title={Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization},
  author={Ma, Xuezhe},
  journal={arXiv preprint arXiv:2009.13586},
  year={2020}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International Conference on Machine Learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@inproceedings{maas2011learning,
  title={Learning word vectors for sentiment analysis},
  author={Maas, Andrew and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies},
  pages={142--150},
  year={2011}
}

@inproceedings{tay2021long,
title={Long Range Arena : A Benchmark for Efficient Transformers },
author={Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=qVyeW-grC2k}
}

@article{su2021roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}

@article{so2021searching,
  title={Searching for Efficient Transformers for Language Modeling},
  author={So, David and Ma{\'n}ke, Wojciech and Liu, Hanxiao and Dai, Zihang and Shazeer, Noam and Le, Quoc V},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={6010--6022},
  year={2021}
}

@inproceedings{press2021train,
  title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  author={Press, Ofir and Smith, Noah and Lewis, Mike},
  booktitle={International Conference on Learning Representations (ICLR-2021)},
  year={2021}
}

@inproceedings{peng2021random,
title={Random Feature Attention},
author={Hao Peng and Nikolaos Pappas and Dani Yogatama and Roy Schwartz and Noah Smith and Lingpeng Kong},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=QtTKTdVrFBB}
}

@article{dai2021coatnet,
  title={Coatnet: Marrying convolution and attention for all data sizes},
  author={Dai, Zihang and Liu, Hanxiao and Le, Quoc V and Tan, Mingxing},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={3965--3977},
  year={2021}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@inproceedings{gu2022efficiently,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  booktitle={International Conference on Learning Representations (ICLR-2022)},
  year={2022}
}

@inproceedings{hua2022transformer,
  title={Transformer quality in linear time},
  author={Hua, Weizhe and Dai, Zihang and Liu, Hanxiao and Le, Quoc},
  booktitle={International Conference on Machine Learning (ICML-2022)},
  pages={9099--9117},
  year={2022},
  organization={PMLR}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{yun2019cutmix,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6023--6032},
  year={2019}
}

@article{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}

@inproceedings{huang2016deep,
  title={Deep networks with stochastic depth},
  author={Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q},
  booktitle={European conference on computer vision},
  pages={646--661},
  year={2016},
  organization={Springer}
}

@inproceedings{hoffer2020augment,
  title={Augment your batch: Improving generalization through instance repetition},
  author={Hoffer, Elad and Ben-Nun, Tal and Hubara, Itay and Giladi, Niv and Hoefler, Torsten and Soudry, Daniel},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8129--8138},
  year={2020}
}

@inproceedings{cubuk2020randaugment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={702--703},
  year={2020}
}

@inproceedings{zhong2020random,
  title={Random erasing data augmentation},
  author={Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  pages={13001--13008},
  year={2020}
}

@article{gu2022parameterization,
  title={On the Parameterization and Initialization of Diagonal State Space Models},
  author={Gu, Albert and Gupta, Ankit and Goel, Karan and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2206.11893},
  year={2022}
}


@article{warden2018speech,
  title={Speech commands: A dataset for limited-vocabulary speech recognition},
  author={Warden, Pete},
  journal={arXiv preprint arXiv:1804.03209},
  year={2018}
}

@inproceedings{
merity2017pointer,
title={Pointer Sentinel Mixture Models},
author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=Byj72udxe}
}

@inproceedings{
hutter2006human,
author={Marcus Hutter},
title={The human knowledge compression contest.},
year={2006},
url={http://prize.hutter1.net/}
}

@inproceedings{baevski2018adaptive,
  title={Adaptive Input Representations for Neural Language Modeling},
  author={Baevski, Alexei and Auli, Michael},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{loshchilov2019decoupled,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{al2019character,
  title={Character-level language modeling with deeper self-attention},
  author={Al-Rfou, Rami and Choe, Dokook and Constant, Noah and Guo, Mandy and Jones, Llion},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={3159--3166},
  year={2019}
}

@inproceedings{khandelwal2019generalization,
  title={Generalization through Memorization: Nearest Neighbor Language Models},
  author={Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{
grave2017improving,
title={Improving Neural Language Models with a Continuous Cache},
author={Edouard Grave and Armand Joulin and Nicolas Usunier},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=B184E5qee}
}

@article{roy2021efficient,
  title={Efficient Content-Based Sparse Attention with Routing Transformers},
  author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={53--68},
  year={2021}
}

@article{liu2021pay,
  title={Pay attention to mlps},
  author={Liu, Hanxiao and Dai, Zihang and So, David and Le, Quoc V},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={9204--9215},
  year={2021}
}

@inproceedings{lei2021attention,
  title={When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute},
  author={Lei, Tao},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={7633--7648},
  year={2021}
}

@article{yarotsky2017error,
  title={Error bounds for approximations with deep ReLU networks},
  author={Yarotsky, Dmitry},
  journal={Neural Networks},
  volume={94},
  pages={103--114},
  year={2017},
  publisher={Elsevier}
}

@article{park2020minimum,
  title={Minimum width for universal approximation},
  author={Park, Sejun and Yun, Chulhee and Lee, Jaeho and Shin, Jinwoo},
  journal={arXiv preprint arXiv:2006.08859},
  year={2020}
}

@article{mehta2022long,
  title={Long range language modeling via gated state spaces},
  author={Mehta, Harsh and Gupta, Ankit and Cutkosky, Ashok and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2206.13947},
  year={2022}
}

@inproceedings{romero2021ckconv,
  title={CKConv: Continuous Kernel Convolution For Sequential Data},
  author={Romero, David W and Kuzina, Anna and Bekkers, Erik J and Tomczak, Jakub Mikolaj and Hoogendoorn, Mark},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{romero2021flexconv,
  title={FlexConv: Continuous Kernel Convolutions With Differentiable Kernel Sizes},
  author={Romero, David W and Bruintjes, Robert-Jan and Tomczak, Jakub Mikolaj and Bekkers, Erik J and Hoogendoorn, Mark and van Gemert, Jan},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{romero2022towards,
  title={Towards a General Purpose CNN for Long Range Dependencies in ND},
  author={Romero, David W and Knigge, David M and Gu, Albert and Bekkers, Erik J and Gavves, Efstratios and Tomczak, Jakub M and Hoogendoorn, Mark},
  journal={arXiv preprint arXiv:2206.03398},
  year={2022}
}
%%%%%%%%%%%%% END OF MEGA

%%%%%%%%%%%%% BEGIN OF INTRODUCTION
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{zaheer2020big,
  title={Big Bird: Transformers for Longer Sequences.},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  booktitle={NeurIPS},
  year={2020}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@inproceedings{tay2020sparse,
  title={Sparse sinkhorn attention},
  author={Tay, Yi and Bahri, Dara and Yang, Liu and Metzler, Donald and Juan, Da-Cheng},
  booktitle={International Conference on Machine Learning},
  pages={9438--9447},
  year={2020},
  organization={PMLR}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{brown2020language,
    title={Language Models are Few-Shot Learners},
    author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    year={2020},
    eprint={2005.14165},
    journal={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@article{narang2021transformer,
  title={Do Transformer Modifications Transfer Across Implementations and Applications?},
  author={Narang, Sharan and Chung, Hyung Won and Tay, Yi and Fedus, William and Fevry, Thibault and Matena, Michael and Malkan, Karishma and Fiedel, Noah and Shazeer, Noam and Lan, Zhenzhong and others},
  journal={arXiv preprint arXiv:2102.11972},
  year={2021}
}

@inproceedings{veach1995optimally,
  title={Optimally combining sampling techniques for Monte Carlo rendering},
  author={Veach, Eric and Guibas, Leonidas J},
  booktitle={Proceedings of the 22nd annual conference on Computer graphics and interactive techniques},
  pages={419--428},
  year={1995}
}

@inproceedings{williams2001using,
  title={Using the Nystr{\"o}m method to speed up kernel machines},
  author={Williams, Christopher and Seeger, Matthias},
  booktitle={Proceedings of the 14th annual conference on neural information processing systems},
  number={CONF},
  pages={682--688},
  year={2001}
}

@article{drineas2005nystrom,
  title={On the Nystr{\"o}m Method for Approximating a Gram Matrix for Improved Kernel-Based Learning.},
  author={Drineas, Petros and Mahoney, Michael W and Cristianini, Nello},
  journal={journal of machine learning research},
  volume={6},
  number={12},
  year={2005}
}
%%%%%%%%%%%%% END OF INTRODUCTION

%%%%%%%%%%%%% BEGIN OF COSFORMER
@article{titsias2016one,
  title={One-vs-each approximation to softmax for scalable estimation of probabilities},
  author={Titsias, Michalis K},
  journal={arXiv preprint arXiv:1609.07410},
  year={2016}
}

@article{gao2017properties,
  title={On the properties of the softmax function with application in game theory and reinforcement learning},
  author={Gao, Bolin and Pavel, Lacra},
  journal={arXiv preprint arXiv:1704.00805},
  year={2017}
}

@article{jang2016categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}

@inproceedings{clark2019does,
  title={What Does BERT Look at? An Analysis of BERT’s Attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  booktitle={Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={276--286},
  year={2019}
}

@inproceedings{kovaleva2019revealing,
  title={Revealing the Dark Secrets of BERT},
  author={Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={4365--4374},
  year={2019}
}
%%%%%%%%%%%%% END OF COSFORMER
